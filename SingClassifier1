import torch #ядро библиотеки, работающее с тензорами (умными массивами).
import torch.nn as nn #содержит «кирпичики» для сборки слоев сети.
import torch.optim as optim #содержит алгоритмы (оптимизаторы), которые подправляют веса сети, чтобы она меньше ошибалась.
#Нейросети нужны примеры для обучения. Создадим 1000 случайных чисел. Если число >0, ответом будет 1, если <0 — ответ 0.
#Создаем 1000 случайных чисел
X = torch.randn(1000, 1) #создание таблицы (тензора) из 1000 строк и 1 столбца, заполненной случайными числами.
# Создаем правильные ответы (лейблы): 1 если число > 0, иначе 0
y = (X > 0).float()  #Она превращает массив случайных чисел в массив из нулей и единиц. Если больше 0, то True
#создаем класс-чертеж нашей сети.
class SimpleNet(nn.Module): #в скобках родитель класса, добавляющий набор готовых умений
    def __init__(self):         #В методе __init__ вы закладываете фундамент и структуру своей нейросети — определяете, сколько в ней будет нейронов и как они будут связаны, прежде чем она начнет делать какие-либо вычисления.
        super().__init__() #super() обращается к «родителю» вашего класса. Как бы заглянуть внутрь главного класса nn.Module из библиотеки PyTorch    .__init__() Это запуск конструктора этого самого «родителя».
                           # Когда вы вызываете super().__init__(), PyTorch внутри вашей модели: Создает систему отслеживания весов (параметров).  Подготавливает механизмы для переноса нейросети на видеокарту (GPU).
                           #Настраивает систему автоматического расчета производных (чтобы сеть могла учиться).  Регистрирует внутренние модули.
        # Входной слой: принимает 1 число, выдает 5 промежуточных значений
        self.hidden = nn.Linear(1, 5)   #self это наша нейронка.  hidden назвоние переменной.  nn.Linear Это тип слоя. (Линейный слой) Математически он делает простую вещь с входящим числом x: y = x * W + b
                                                             # Где W — это веса (weights), а b — смещение (bias).   1 это количество чисел, которые придут на вход. 5 количество нейронов в этом слое. Мы превращаем одно число в 5 разных сигналов.
                                                             # Представьте, что 1 число — это один «факт». Когда мы передаем его на 5 нейронов, каждый из этих нейронов начинает смотреть на этот факт под своим углом (у каждого свои веса).
        # Функция активации: делает нейросеть «умнее», позволяя решать нелинейные задачи
        self.relu = nn.ReLU()  #добавляет в вашу нейросеть функцию активации. функции активации — это её «логика».  ReLU это самая популярная функция активации в современных нейросетях. Если число отрицательно, то превращает в 0, начаче оставляет как есть
                               #Она Вносит нелинейность, позволяет нейросети решать сложные задачи, которые нельзя описать просто прямой линией. Эффект «включения/выключения» нейрона: Если результат вычисления отрицательный, ReLU «выключает» этот нейрон (выдает 0). Это имитирует работу реального биологического нейрона
                               #self.hidden выдает 5 чисел self.relu проходит по этим 5 числам и «обнуляет» все минусы.
        # Выходной слой: из 5 значений делает 1 (наш прогноз)
        self.output = nn.Linear(5, 1)  #self.output название модели и название переменой nn.Linear Он работает по тому же принципу, что и первый слой: берет входящие данные, умножает на веса и добавляет смещение.
                                                            #5 это количество входящих сигналов 1 количество чисел, что получим на выходе Поскольку наша задача — просто определить, является ли число положительным (выдать ответ «да» или «нет»), нам достаточно одного числа.
                                                            #Представьте, что 5 нейронов предыдущего слоя провели «совещание». Каждый из них высказал свое мнение (5 чисел). Слой self.output — это как директор, который выслушивает все 5 мнений, взвешивает их (умножает каждое на свой вес) и выносит один вердикт в виде одного числа.
                                                            #Если итоговое число будет большим — сеть считает число положительным. Если очень маленьким (отрицательным) — отрицательным.
        # Сигмоида превращает число в вероятность от 0 до 1
        self.sigmoid = nn.Sigmoid()  #добавляет в нейросеть финальный «фильтр», который превращает любое число в понятную вероятность.
                                     #Функция Сигмоида берет любое число от -бесконечности до +infty и «сжимает» его в строгий диапазон от 0 до 1.   Если на вход пришло очень большое положительное число (например, 100, на выходе будет почти 1. Если отрицательное, то почти 0. 0 это 0.5
                                     #нам неудобно получать от нейросети ответы типа 5.7 или -10.2. Нам нужен ответ в формате: «На сколько процентов ты уверена, что это число положительное?».
                                     #0.95 - 95% уверена  0.5 сомневается
                                     #ReLU используется внутри сети (между слоями), чтобы нейроны могли «думать» и строить сложные зависимости. Она не ограничивает числа сверху.
                                     #Sigmoid используется обычно на самом последнем этапе, чтобы привести ответ к красивому виду вероятности.
                                     #Слой output выдает какое-то сырое число (например, 2.5). Sigmoid берет это 2.5 и превращает его в 0.92. Вы смотрите на 0.92 и понимаете: «Ага, нейросеть считает, что число с большой вероятностью положительное!».

    def forward(self,x):
        # Описываем путь данных через слои
        x = self.relu(self.hidden(x))   #forward — это инструкция по сборке, а эта строка — её первый шаг.
                                        #self.hidden(x) Входные данные x (наше число) попадают в скрытый линейный слой, который мы создали в __init__.
                                        #Здесь число умножается на 5 разных весов. На выходе из этой скобки мы получаем 5 новых чисел.
                                        #self.relu(...) Результат работы слоя (те самые 5 чисел) тут же передается в функцию активации ReLU.
                                        #ReLU проверяет каждое из этих 5 чисел Все так же проверяет отрицательные
                                        #Мы перезаписываем переменную x. Теперь в ней хранятся уже не исходные данные, а «осмысленные» нейронами и очищенные от минусов значения. Мы подготовили их для передачи на следующий слой.
                                        #self.hidden(x) — это первичная перегонка (мы разложили её на 5 фракций).
                                        #self.relu(...) — это фильтрация (мы убрали вредные примеси/отрицательные значения).
                                        #x = ... — теперь у нас в руках очищенное топливо, готовое для финальной обработки.
        x = self.sigmoid(self.output(x)) # прогоняем через прошлый def в котором все описано
        return x  #Через return она выдает вам вероятность.
model = SimpleNet()  #вы создаете конкретный экземпляр (объект) этой сети. Теперь с переменной model можно работать: обучать её или подавать в неё данные.

#Чтобы сеть училась, ей нужно знать, как сильно она ошибается, и кто-то должен её исправлять.
# Функция потерь (Loss): измеряет разницу между прогнозом и реальностью
criterion = nn.BCELoss()  #это назначение «судьи», который будет оценивать работу вашей нейросети.  Сигмоида выдает вероятность (от 0 до 1).
                        #BCELoss берет эту вероятность и сравнивает её с реальной меткой (0 или 1).
                        #Она очень строго «штрафует» нейросеть за самоуверенность в неправильном ответе.
# Оптимизатор: подкручивает нейроны, основываясь на ошибке
optimizer = optim.SGD(model.parameters(), lr=0.1)  #это назначение «тренера» для вашей нейросети.
                                                    #optim.SGD Это классический и самый надежный метод обучения. Как он работает: он смотрит на «склон» горы (ошибку) и делает шаг вниз, в сторону, где ошибка меньше.
                                                    #Метод parameters() передает оптимизатору список всех весов и смещений внутри вашей модели SimpleNet. Без этого оптимизатор не будет знать, что именно ему разрешено менять.
                                                    #lr=0.1 Это скорость обучения — самый важный параметр. Он определяет, насколько «размашистыми» будут шаги оптимизатора.
#Это самый важный процесс. Мы прогоняем данные через сеть много раз (эпох).
for epoch in range(500):
    # 1. Сброс старых расчетов (градиентов)
    optimizer.zero_grad()  #чтобы забыть старые ошибки.
    # 2. Прямой проход: модель делает предсказание
    predictions = model(X)
    # 3. Считаем ошибку
    loss = criterion(predictions, y)
    # 4. Обратное распространение: вычисляем, какие веса виноваты в ошибке
    loss.backward()  #Взять значение ошибки. Пройтись назад по всей нейросети — от последнего слоя к самому первому. С помощью производных вычислить, насколько сильно каждый конкретный вес виноват в итоговой ошибке.
    # 5. Шаг оптимизатора: исправляем веса
    optimizer.step()  #Он обновляет каждый вес по формуле:  Вес=Вес-(Скорость_Обучения * Градиент)  Градиент указывает направление (в какую сторону менять). Learning Rate (lr) указывает размер шага (насколько сильно менять).
    if epoch %100 == 0:
        print(f'Эпоха {epoch}, Ошибка: {loss.item():.4f}')  #Выводим отчеты через каждые 100
                                                            #Переменная loss — это не просто число, а тензор (объект PyTorch), который хранит в себе историю вычислений и градиенты. Метод .item() «вытаскивает» из этого объекта чистое число Python (float).
                                                            # Зачем это нужно: Если печатать просто loss, вы увидите громоздкое tensor(0.1234, grad_fn=<...>). Метод .item() делает вывод чистым.
                                                            #:.4f Это инструкция, как именно показать число ошибки:  : — начало инструкции.  .4 — оставить ровно 4 знака после запятой.  f — формат float (десятичная дробь).
                                                            #Если число уменьшается — нейросеть учится.  Если число растет — вы поставили слишком большой learning rate.  Если число не меняется — данные плохие или архитектура сети не подходит.
#Теперь проверим, чему научилась наша сеть.
test_number = torch.tensor([[ -5.5 ]])  # Попробуем число -5.5
model.eval() # Режим проверки  во время обучения этот слой случайно «выключает» некоторые нейроны, чтобы сеть не зубрила ответы. В режиме eval() все нейроны включаются обратно для получения максимально точного результата.
                #этот слой во время обучения считает среднее значение по всей «пачке» данных. В режиме eval() он перестает считать статистику и использует уже накопленные знания.
with torch.no_grad():  #это команда экономии ресурсов и отключения «режима обучения» на уровне расчетов. Она говорит PyTorch: «Просто считай результат, не запоминай путь!».
                       #with создание зоны с особыми правилами
    prediction = model(test_number)
    print(f'Результат для -5.5: {prediction.item():.4f}')
    # Значение близкое к 0 означает 'отрицательное'
